% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calcFidelity.R
\name{calcFidelity}
\alias{calcFidelity}
\title{Calculate fidelity metric(s) for a classifier}
\usage{
calcFidelity(X, truthVar, predVar, groupVars = NULL,
  methods = fidelityMethods(), aggregate = c("micro", "macro"),
  positiveLabel = NULL)
}
\arguments{
\item{X}{data.frame containing the predicted and true classes for
each group. See details}

\item{truthVar}{Character string indicating the column name in \code{X} that
contains the true class of the observations}

\item{predVar}{Character string indicating the column name in \code{X} that
contains the class predicted by the classifier}

\item{groupVars}{Character vector indicating one or more column names in \code{X}
that define the groups for which the fidelity metrics will be separately calculated}

\item{methods}{vector of fidelity methods that will be used. For a list of
available fidelity methods, see fidelityMethods()}

\item{aggregate}{string that uniquely indicates the type of aggregation; by default,
\code{micro}. See details}

\item{positiveLabel}{For binary classification only, the value of the class label that corresponds to
a positive response. Should be the same type (character, numeric, or integer) as the true and predicted
labels.}
}
\value{
a list that contains two components: \code{aggregate} and \code{byClass}. The former
contains the aggregate fidelity results, and the latter contains the fidelity
results for each class within each signature.
}
\description{
With a vector of predicted classes and corresponding
vector of truth classes, calculate the fidelity of a classifier using one or
more metrics derived from the confusion matrix (e.g. accuracy, sensitivity, false
positive rate, etc.).
}
\details{
The data.frame \code{X} must have has at least two columns: a column containing the
true class, and another column containing the predicted class.  Optionally, it may
have other columns that indicate groups for which the fidelity metrics will be
calculated separately.  If \code{X[,truthVar]} and \code{X[,predVar]} are both factors,
they must have the same levels.  If they are not both factors, they must at least
be of the same type (character, numeric, integer, etc.) and they are converted
to factors with the levels being defined as the union of the unique values in
\code{X[,truthVar]} and \code{X[,predVar]}. The rows of \code{X} correspond to each
observation, or example, for which a classification is made.

The two aggregate score options are the macro- and micro-aggregate (average)
scores. The macro-aggregate score is the arithmetic mean of the binary scores
for each class. The micro-aggregate score is a weighted average of each class'
binary score, where the weights are determined by the sample sizes for each
class. By default, we use the micro-aggregate score because it is more robust,
but the macro-aggregate score might be more intuitive to some users.
If the classification is binary, aggregation is still performed, but the user
may be interested in only one of the classes which is available in \code{byClass}.

The available fidelity methods can be viewed with
\code{\link{fidelityMethods}}.
}
\examples{
# Load data
data(exampleSignatures)

# Calculate the TPR and FPR
calcFidelity(exampleSignatures, truthVar = "truthClass", predVar = "predictedClass",
             groupVars = "signatureID", methods = c("TPR","FPR"))


# An example where there is more than one grouping variables
# Create a simpler dataset
d <- exampleSignatures[exampleSignatures$signatureID \%in\% 1:4,]
d[d$signatureID \%in\% c(1,2), "gender"] <- "Male"
d[d$signatureID \%in\% c(3,4), "gender"] <- "Female"
d[d$signatureID \%in\% c(1,3), "nationality"] <- "American"
d[d$signatureID \%in\% c(2,4), "nationality"] <- "French"
d <- d[,-which(colnames(d) == "signatureID")]

# Now calculate using the grouping variables
y <- calcFidelity(d, groupVars = c("gender", "nationality"))
y
}

