% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fidelityMethods.R
\name{Fscore}
\alias{Fscore}
\title{Computes classification F-score from the confusion matrix summary based on a
set of predicted and truth classes for a signature.}
\usage{
Fscore(confusionSummary, aggregate = c("micro", "macro"), beta = 1)
}
\arguments{
\item{confusionSummary}{list containing the confusion summary for a set of
classifications}

\item{aggregate}{string that indicates the type of aggregation; by default,
micro. See details.}

\item{beta}{numeric coefficient that controls the F-score's weight of the
precision and recall}
}
\value{
list with the accuracy measure for each class as well as the macro-
and micro-averages (aggregate measures across all classes).
}
\description{
For each class, we calculate the classification F-score in order to
summarize its performance for the signature. We compute one of two aggregate
scores, to summarize the overall performance of the signature.
}
\details{
The F-score is a weighted average of the precision and recall. The weight of
this average is controlled by a coefficient beta, which is defaulted to 1.
For the default, the F-score is the harmonic mean of the precision and recall.

The two aggregate score options are the macro- and micro-aggregate (average)
scores. The macro-aggregate score is the arithmetic mean of the binary scores
for each class. The micro-aggregate score is a weighted average of each class'
binary score, where the weights are determined by the sample sizes for each
class. By default, we use the micro-aggregate score because it is more robust,
but the macro-aggregate score might be more intuitive to some users.

The F-score measure ranges from 0 to 1 with 1 being the optimal value.
}

