% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/piclapply.R
\name{piclapply}
\alias{piclapply}
\title{High performance computing (HPC) parallelization of lapply() on PIC}
\usage{
piclapply(X, FUN, account, ..., packages = NULL, header.file = NULL,
  needed.objects = NULL, needed.objects.env = parent.frame(),
  keep.names = TRUE, jobName = "piclapply", numNodes = 2,
  partition = "slurm", time.limit.mins = 30, check.interval.sec = 30,
  tarball = NULL, parseJob.args = NULL, remove.working.dir = TRUE,
  tmp.file.list = NULL, email.notification = NULL, verbose = FALSE)
}
\arguments{
\item{X}{The list, each element of which will be the input to \code{FUN}}

\item{FUN}{A function whose first argument is an element of list \code{X}}

\item{account}{A character string indicating the PIC account (e.g. "CSI",
"SDI", "USERS") that will charged for the computing time.}

\item{packages}{Character vector giving the names of packages that will be
loaded in each new instance of R. If \code{NULL}, no packages are loaded for
each new instance of R.}

\item{header.file}{Text string indicating a file that will be initially
sourced prior calling \code{\link{lapply}} in order to create an
'environment' that will satisfy all potential dependencies for \code{FUN}.
If \code{NULL}, no file is sourced.}

\item{needed.objects}{Character vector giving the names of objects which
reside in the evironment specified by \code{needed.objects.env} that may be
needed by \code{FUN}.  These objects are loaded into the GLOBAL ENVIRONMENT
of each new instance of R that is launched.  If \code{NULL}, no additional
objects are passed the the global environment of the various R instances.}

\item{needed.objects.env}{Environment where \code{needed.objects} reside.
This defaults to the environment in which \code{piclapply} is called.}

\item{keep.names}{Logical, that when TRUE, causes the original names of the list to
be retained on the output list if possible.  This also ensures the output list
is returned in the same order as the input list.}

\item{jobName}{Text string indicating the prefix for temporary files and folders that
will be created while launching the separate instances of R.}

\item{numNodes}{Integer indicating the number of nodes requested for
parallel processing. For example, each slurm node on constance has 24 cores. So, for example,
requesting 3 nodes is equivalent to requesting 72 parallel jobs.}

\item{partition}{Character string indicating the PIC partition to which the
job should be submitted. Defaults to \code{slurm},
which is the partition for 'usual' jobs.  The \code{short} partition is for
testing and code development for short jobs of 1 hour or less.
The \code{gpu}
partition contains nodes with GPU capability, and \code{long} is for long jobs,
Use the system command
\code{sview} to see a graphical depiction of the entire cluster and to see
more detailed information about the partitions, and which nodes are available on
a given partition.}

\item{time.limit.mins}{Integer indicating the time limit (in minutes) of the
SLURM batch job. If the SLURM job exceeds this limit it will be canceled.
The tradeoff is that jobs with shorter limits are likely to be scheduled for launch
sooner.}

\item{check.interval.sec}{The number of seconds to wait between checking to
see whether the SLURM batch job has completed.}

\item{tarball}{A text string indicating the file (ending in .tar.gz)
where all the files used to create and support the SLURM job will be stored.
If \code{NULL}, a file of the form 'jobName_output_####.tar.gz' will be
stored in the working directory.  If \code{tarball = "none"}, no
tarball will be created.}

\item{parseJob.args}{A named list of optional (named) arguments to
\code{\link{parseJob}} in the \code{Smisc} package. These arguments govern how
the list \code{X} will be subdivided for parallel processing. If \code{parseJob.args = NULL},
the default arguments of \code{\link{parseJob}} are used.}

\item{remove.working.dir}{\code{= TRUE} requests the temporary working directory of
the SLURM job files be deleted on successful completion.  If the SLURM job
fails, this directory will not be removed.}

\item{tmp.file.list}{A character string indicating the name of a list containing the path of the
temporary folder and the output tarball file.  This list is
assigned to the global environment.  If NULL, the list is not written to
the global environment.}

\item{email.notification}{Text string containing an email address to which
an email will be sent upon completion of the SLURM job.  If \code{NULL}, no
email will be sent.}

\item{verbose}{\code{= TRUE} prints details (and timing) of the job}

\item{\dots}{Additional named arguments to \code{FUN}}
}
\value{
A list equivalent to that returned by \code{lapply(X, FUN, ...)}.
}
\description{
Parses a list into subsets and submits a separate R job using lapply() for each subset.
Jobs are executed using parallelized high
performance computing on the PNNL institutional computing cluster (PIC).
The parallel jobs are instantiated on multiple nodes/cores
using a SLURM batch job.
}
\details{
\code{piclapply} applies \code{FUN} to each element of the list \code{X} by
parsing the list into sublists of equal (or almost equal) size (using
\code{\link{parseJob}}) and then applying \code{FUN} to each sublist using
\code{\link{lapply}}. The list, \code{X}, will be parsed using \code{\link{parseJob}} into
\code{n * numNodes} sublists where \code{n} is the number of CPU cores on the requested nodes.
Each of the sublists will be processed by
\code{\link{lapply}} on a separate core running its own instance of R.

If the length of the list is shorter than the number of requested cores (which
is \code{n * numNodes}), then the number of nodes is reassigned to a value
that will utilize full nodes to the extent possible.

After calling \code{piclapply}, it is also good practice to log into one (or
more) of the nodes that are assigned by SLURM and verify (using \code{top})
that all of the cores are computing as expected.

After the jobs complete, the output lists are reassembled into a single output list.
The order of names of the output list will match those of the input list, provided the two lists
have the same lenth and \code{keep.names = TRUE}.

A number of objects are made available in the global environment of each
instance of R.  These objects can be used by \code{FUN} if needed.  They
are:

\itemize{

\item \code{process.id}: An integer uniquely identifying the process of R
that is running. This will range from
0 to \code{n * numRprocesses - 1}.
\item \code{numRprocesses}: The total
number of R instances launched by SLURM.  In general, this will be \code{n * numNodes}.
\item \code{wkdir.out}: A character string indicating the directory where output from
each R instance is saved.
}

Each instance of R runs a script that performs the following steps:

\enumerate{

\item The \code{piclapply} package is loaded.

\item Any other packages indicated in the \code{packages} argument are
loaded.

\item The \code{process.id} global variable is assigned (having been passed
in via a command line argument).  This variable identifies the particular
instance of R.

\item The header file (if there is one) is sourced.

\item The R environment file is loaded, which contains the list (\code{X}),
the function (\code{FUN}), any \code{needed.objects}, as well as all other
objects (internally created by \code{piclapply}) that will be needed.

\item The expression \code{pre.process.expression} is evaluated if an object
of that name is present in the global environment. The object
\code{pre.process.expression} may be passed in via the header file or via
\code{needed.objects}.

\item The subset of the indexes of the list \code{X} (that will create the
sublist) is identified for the particular instance of R, using the
\code{process.id} variable.

\item The sublist, \code{X.sub}, of the list \code{X} is created and
\code{X} is removed to save memory.

\item \code{\link{lapply}} is called as follows:
\code{X.sub.out <- lapply(X.sub, FUN.p)}, where \code{FUN.p} is a wrapper to
\code{FUN} containing named arguments.

\item The expression \code{post.process.expression} is evaluated if an
object of that name is present in the global environment.  The object
\code{post.process.expression} may be passed in via the header file or via
\code{needed.objects}.  This provides a mechanism to reduce the output in
\code{X.sub.out} if needed (see Example 3).

\item The list \code{X.sub.out} is saved to a file in \code{wkdir.out} where
it will be collected after all jobs have completed.

\item Warnings are printed.
}
}
\examples{

########################################
# Example 1
########################################

## You must set the PIC account name before launching this example:
## i.e., account <- "PIC_account_name"
\dontshow{
if (exists("account", envir = .GlobalEnv)) {
  account <- get("account", envir = .GlobalEnv)
} else {
   stop("To run this example, you need to assign the PIC account name to 'account' in the Global Environment, as follows:\\n",
       "account <- \\"PIC_account_name\\"")
}
}
# Create a simple list
a <- list(a = rnorm(10), b = rnorm(20), c = rnorm(15), d = rnorm(13), e = rnorm(15), f = rnorm(22))

# Some objects that will be needed by f1:
b1 <- rexp(20)
b2 <- rpois(10, 20)

# The function
f1 <- function(x) mean(x) + max(b1) - min(b2)

# Call piclapply
res.1 <- piclapply(a, f1, account,
                   needed.objects = c("b1", "b2"),
                   jobName = "example.1",
                   numNodes = 1,
                   partition = "short",
                   check.interval.sec = 1,
                   time.limit.mins = 1,
                   tarball = "none",
                   verbose = TRUE)
# Call lapply to check the results
res.2 <- lapply(a, f1)
print(res.2)

# Compare results
all.equal(res.1, res.2)


########################################
# Example 2
########################################

# Create a function that calculates the mean of a normal variate
mean.tmp <- function(list.element, sigma = 1) {
  set.seed(list.element)
  mean(rnorm(500, mean = list.element, sd = sigma))
}

# Create a list of means (and seeds) to operate over
aList <- as.list(0:10000)

# Calculate it without piclapply
res.3 <- lapply(aList, mean.tmp, sigma = 0.5)

res.4 <- piclapply(aList, mean.tmp, account, sigma = 0.5,
                   jobName = "example.2",
                   time.limit.mins = 1,
                   numNodes = 3,
                   partition = "slurm",
                   parseJob.args = list(collate = TRUE, text.to.eval = TRUE),
                   check.interval.sec = 1,
                   tarball = "none",
                   verbose = TRUE)

head(res.3)
tail(res.3)

# These should be the same...
all.equal(res.3, res.4)


########################################
# Example 3:
# A more complicated example with packages,
# header files, and pre- and post-expressions
########################################

# Create the header file

hf <- cat(# Print the process id and the number of R processes to the .Rout file
          "Smisc::pvar(process.id)\\n",

          # Create the pre-process expression by writing it into the header file
          "pre.process.expression <- expression({\\n",

          # Summarize the muscle data set to illustrate using another package (MASS)
          "   Smisc::pvar(numRprocesses)\\n",
          "   data(muscle)\\n",
          "   summary(muscle)\\n",

          "})\\n",

          # List all the objects in the environment
          "print(ls())\\n",

          # Write to the header file
          file = "tmpHeader.R", sep = "")

# Display the header file
Smisc::more("tmpHeader.R")

# This post expression will be passed in via 'needed.arguments'
post.process.expression <- expression({

  # Show original length of X.sub.out
  Smisc::pvar(length(X.sub.out))
  cat("Reducing X.sub.out now...\\n")

  # Calculate the mean for all the observations in the R process to reduce the sublist
  # into a list with 1 element
  X.sub.out <- as.list(mean(unlist(X.sub.out)))

  # Now show its reduced length
  Smisc::pvar(length(X.sub.out))

})

# Create a function that calculates the mean of a normal variate
mean.tmp <- function(list.element, sigma = 1) {

  # Set the seed using the process id, which exists in the global environment of the R instance
  set.seed(process.id)

  # Return the mean
  return(mean(rnorm(500, mean = list.element, sd = sigma)))

}

# Create a list of means to operate over
aList <- as.list(0:10000)

# Run with lots of options
res.4 <- piclapply(aList, mean.tmp, account,
                   sigma = 0.5,
                   header.file = "tmpHeader.R",
                   packages = "MASS",
                   needed.objects = "post.process.expression",
                   jobName = "example.3",
                   time.limit.mins = 1,
                   numNodes = 2,
                   partition = "short",
                   parseJob.args = list(text.to.eval = TRUE),
                   check.interval.sec = 1,
                   tmp.file.list = "tmpFiles",
                   remove.working.dir = FALSE,
                   tarball = "none",
                   verbose = TRUE)

# Due to the post.process expression, this list has relatively few elements: one for each R
# process, whereas the original list had length 10000
length(res.4)
head(res.4)

# Remove the header file
unlink("tmpHeader.R")

# Now print the 'tmpFiles' list object that was written to the global environment
print(tmpFiles)

# Look at the .Rout file from one of the instances
anRoutFile <- dir(file.path(tmpFiles$wkdir, "logs"), full.name = TRUE)[2]
anRoutFile
Smisc::more(anRoutFile)

# Now remove the temporary folders
system(paste("rm -r", tmpFiles$wkdir))
}
\author{
Landon Sego
}
\seealso{
\code{\link{lapply}}, \code{\link{plapply}}, \code{\link{dfplapply}},
\code{\link{showAvailableSystems}}, \code{\link{killSlurm}}
}
\keyword{misc}

